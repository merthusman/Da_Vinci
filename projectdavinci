# ======================================================================================
# SECTION 0: ENVIRONMENT SETUP AND INITIAL IMPORTS
# ======================================================================================

import os
import pickle
import time
import shutil
import warnings
import itertools
import pandas as pd

# Hardware optimizations
os.environ['CUPY_ACCELERATORS'] = 'cub,cutensor'
os.environ['CUPY_TF32'] = '0' # Disable TF32 for numerical stability, similar to Prometheus

import cupy as cp
import numpy as np
from tqdm.auto import tqdm # Using .auto instead of .notebook provides a more general solution
from scipy.signal import find_peaks
# ATTENTION: The detrend function used by Prometheus comes from a different library!
from cupyx.scipy.signal import detrend # We will use this instead of scipy.signal.detrend
from scipy.stats import linregress, gmean
import matplotlib.pyplot as plt
import math # The math library used by Prometheus

DTYPE_COMPLEX = cp.complex64
DTYPE_FLOAT = cp.float32

warnings.filterwarnings("ignore")

# ======================================================================================
# UPDATED AND DETAILED PARTICLE DATABASE
# ======================================================================================
PDG_DATA = {
    # Leptons (Fermion)
    "Electron":   {'mass': 0.511, 'type': 'Lepton (Fermion)'},
    "Muon":       {'mass': 105.7, 'type': 'Lepton (Fermion)'},
    "Tau":        {'mass': 1777,  'type': 'Lepton (Fermion)'},

    # Mesons (Hadron / Boson)
    "Pion (π⁰)":   {'mass': 135.0, 'type': 'Meson'},
    "Pion (π⁺)":   {'mass': 139.6, 'type': 'Meson'},
    "Kaon (K⁺)":   {'mass': 493.7, 'type': 'Meson'},
    "Kaon (K⁰)":   {'mass': 497.6, 'type': 'Meson'},
    "Eta (η)":     {'mass': 547.9, 'type': 'Meson'},
    "Rho (ρ)":     {'mass': 775.3, 'type': 'Meson'},
    "Omega (ω)":   {'mass': 782.7, 'type': 'Meson'},
    "J/psi (J/ψ)": {'mass': 3096.9,'type': 'Meson (Quarkonium)'},

    # Baryons (Hadron / Fermion)
    "Proton":      {'mass': 938.3, 'type': 'Baryon (Fermion)'},
    "Neutron":     {'mass': 939.6, 'type': 'Baryon (Fermion)'},
    "Lambda (Λ⁰)": {'mass': 1115.7,'type': 'Baryon (Fermion)'},
    "Sigma (Σ⁺)":  {'mass': 1189.4,'type': 'Baryon (Fermion)'},
    "Delta (Δ⁺⁺)": {'mass': 1232,  'type': 'Baryon (Fermion)'},

    # Vector Bosons
    "W Boson":    {'mass': 80379, 'type': 'Vector Boson'},
    "Z Boson":    {'mass': 91187, 'type': 'Vector Boson'},

    # Scalar Boson
    "Higgs Boson":{'mass': 125100,'type': 'Scalar Boson'},
}

# ======================================================================================
# SECTION 1A: CLIFFORD ALGEBRA FUNDAMENTALS
# ======================================================================================
class CliffordAlgebra:
    def __init__(self, p, q):
        self.p, self.q, self.dim = p, q, p + q
        self.num_blades = 2**self.dim
        
        neg_mask_cpu = np.int32(0)
        for i in range(self.p, self.dim):
            neg_mask_cpu |= (1 << i)
        self.neg_metric_mask = cp.asarray(neg_mask_cpu)

        self.grades = cp.asarray([bin(i).count('1') for i in range(self.num_blades)], dtype=cp.int32)
        self.blade_indices = {g: cp.where(self.grades == g)[0].astype(cp.int32) for g in range(self.dim + 1)}
        
        self.e_vecs = cp.zeros((self.dim, self.num_blades), dtype=cp.complex64)
        for i in range(self.dim):
            self.e_vecs[i, 1 << i] = 1.0

# ======================================================================================
# SECTION 1B: EMERGENT PHENOMENA ANALYZER
# ======================================================================================

class EmergentPhenomenaAnalyzer:
    def __init__(self, alg, final_psi_state):
        self.alg = alg
        self.psi = final_psi_state

    def analyze_symmetry_breaking(self):
        grade_norms = []
        for g in range(self.alg.dim + 1):
            indices = self.alg.blade_indices.get(g)
            if indices is not None and indices.size > 0:
                norm_sq = cp.sum(cp.real(self.psi[..., indices] * cp.conj(self.psi[..., indices])))
                grade_norms.append(norm_sq.get().item())
            else:
                grade_norms.append(0)

        report = {f"grade_{g}": norm for g, norm in enumerate(grade_norms)}

        if not all(np.isfinite(grade_norms)):
            return 10.0, report

        total_norm = sum(grade_norms)
        if total_norm < 1e-9:
            return 0.0, report

        probabilities = np.array([n / total_norm for n in grade_norms])
        
        std_dev = np.std(probabilities)
        imbalance_score = min((std_dev / 0.3) * 10.0, 10.0) 
        
        return imbalance_score, report

    def analyze_structural_complexity(self):
        if not cp.all(cp.isfinite(self.psi)).get().item():
            return 1e6

        laplacian_psi = cp.roll(self.psi, 1, axis=1) + cp.roll(self.psi, -1, axis=1) + \
                        cp.roll(self.psi, 1, axis=0) + cp.roll(self.psi, -1, axis=0) - 4.0 * self.psi
        
        if not cp.all(cp.isfinite(laplacian_psi)).get().item():
            return 1e6

        complexity = cp.mean(cp.real(laplacian_psi * cp.conj(laplacian_psi))).get().item()
        
        if not np.isfinite(complexity):
            return 1e6

        return np.log1p(complexity) * 10
        
    @staticmethod
    def analyze_particle_spectrum_hybrid(time_series_gpu_dict, dt):
        all_particles = {}
        ts_data_map = time_series_gpu_dict.get('all_grades', {})
        time_series_gpu = ts_data_map
    
        for grade in range(time_series_gpu.shape[0]):
            signal_gpu = time_series_gpu[grade]
    
            if not cp.all(cp.isfinite(signal_gpu)) or signal_gpu.size < 50:
                continue
    
            signal_gpu = detrend(signal_gpu, type='linear')
            signal_gpu -= cp.mean(signal_gpu)
    
            if cp.all(cp.abs(signal_gpu) < 1e-9):
                continue
    
            N = len(signal_gpu)
            xf_gpu = cp.fft.rfftfreq(N, dt)
            power_gpu = cp.abs(cp.fft.rfft(signal_gpu))
    
            if xf_gpu.size > 0:
                power_gpu[xf_gpu < 0.01] = 0
    
            max_power_val = cp.max(power_gpu).get().item()
            if not power_gpu.size > 0 or max_power_val < 1e-9:
                continue
    
            prominence_threshold = 1e-6 # Ultra-sensitive mode
    
            power_cpu = power_gpu.get()
            xf_cpu = xf_gpu.get()
    
            peaks, props = find_peaks(power_cpu, prominence=prominence_threshold, distance=10)
    
            for i, peak_idx in enumerate(peaks):
                freq = xf_cpu[peak_idx]
                peak_power = props['prominences'][i]
                if freq not in all_particles or peak_power > all_particles[freq]['power']:
                    # We store the power information in an intermediate dictionary
                    all_particles[freq] = {'power': peak_power, 'channel': f'grade_{grade}'}
                
        if not all_particles:
            return []
        
        
        return sorted([{'id': i + 1, 'freq': freq, 'channel': info['channel'], 'power': info['power']}
                       for i, (freq, info) in enumerate(all_particles.items())],
                      key=lambda p: p['freq'])
# ======================================================================================
# SECTION 1C: KERNEL EVOLUTION (AresLegacyEngine) - SAFE/FAST MODE (CLEAN VERSION)
# ======================================================================================
class AresLegacyEngine:
    def __init__(self, size, alg, params, base_noise_level):
        # Core attributes
        self.size, self.alg, self.params, self.base_noise_level = size, alg, params, base_noise_level
        self.stream = cp.cuda.Stream(non_blocking=True)

        # --- ALL GPU BUFFERS MUST BE DEFINED HERE ---
        self.psi = cp.zeros((size, size, alg.num_blades), dtype=DTYPE_COMPLEX)
        self.pi = cp.zeros_like(self.psi)
        
        # Buffer to correct the existing error:
        self.force_buffer = cp.zeros_like(self.psi)
        
        # Workspace buffers required for `evolve_feynman`:
        self.workspace1 = cp.zeros_like(self.psi)
        self.workspace2 = cp.zeros_like(self.psi)
        self.temp_workspace = cp.zeros_like(self.psi)
        self.dt_gpu = cp.zeros(1, dtype=cp.float32)
        # -----------------------------------------------
        
        # For random states (to be used in the relax method)
        self._init_random_states()
        
        # Set up CUDA kernels and engine parameters
        self._build_kernels()
        self._set_engine_params(params)
    def _set_engine_params(self, params_dict):
        """Sets the engine's physics parameters in a GPU-compatible format."""
        self.lambda_d = cp.float32(params_dict.get('lambda_d', 1.0))
        self.lambda_p_global = cp.float32(params_dict.get('lambda_p_global', 0.1))
        # Note: Prometheus's parameter names may differ; adjust according to your main model.
        # This code assumes Prometheus's naming convention.
        self.lambda_p_gpu = cp.array([params_dict.get(f'lambda_p_g{g}', 0.0) for g in range(self.alg.dim + 1)], dtype=cp.float32)
        self.lambda_b_gpu = cp.array([params_dict.get(f'lambda_b_g{g}', 0.0) for g in range(self.alg.dim + 1)], dtype=cp.float32)
        self.lambda_tension = cp.float32(params_dict.get('lambda_tension', 0.0))
        self.lambda_complexity = cp.float32(params_dict.get('lambda_complexity', 0.0))
        self.lambda_resonance = cp.float32(params_dict.get('lambda_resonance', 0.0))
        self.damping_relax = cp.float32(params_dict.get('damping_relax', 0.05))
        self.damping_evolve = cp.float32(params_dict.get('damping_evolve', 1e-6))
    
    def _init_random_states(self):
        """Generates random states for thermal noise in the relaxation method."""
        total_elements = self.psi.size
        self.rand_states = cp.random.randint(0, 2**32-1, size=total_elements, dtype=cp.uint32)


    def _build_kernels(self):
        """
        ### - COMPLETE, UNABRIDGED, AND CORRECTED KERNEL COMPILATION METHOD ###
        This method corrects the physical error in the Feynman experiment AND includes the missing helper functions
        that caused compilation errors. This is a complete and self-contained code block.
        """
        n_blades = self.alg.num_blades
        dim = self.alg.dim
        max_entropy_val = math.log(dim + 1) if dim > 0 else 1.0

        # COMPLETE CUDA CODE CONTAINING ALL NECESSARY FUNCTIONS
        cuda_code = f'''
        typedef float2 cfloat;

        // ==============================================================================
        // SECTION A: HELPER DEVICE FUNCTIONS (ALL)
        // ==============================================================================
        
        // Random number generator (lcg) and uniform distribution function
        __device__ unsigned int lcg(unsigned int &state) {{
            state = 1664525u * state + 1013904223u;
            return state;
        }}
        
        __device__ float lcg_uniform(unsigned int &state) {{
            return (float)lcg(state) / (float)0xffffffff;
        }}

        // Bit counting function (used by the old gp_kernel)
        __device__ int countSetBits(int n) {{
            int count = 0;
            while (n > 0) {{
                n &= (n - 1);
                count++;
            }}
            return count;
        }}
        
        // REPORT SECTION 4.2.2: HIGH-PERFORMANCE SIGN CALCULATION (BIT MANIPULATION)
        __device__ __forceinline__ float calculate_sign_on_the_fly(int i, int j, int neg_mask, int dim) {{
            int swaps = 0;
            unsigned int temp_j = j;
            while(temp_j) {{
                int k = __ffs(temp_j) - 1;
                swaps += __popc(i & ~((1u << (k + 1)) - 1));
                temp_j &= temp_j - 1;
            }}
            float sign = (swaps % 2 == 0) ? 1.0f : -1.0f;
            int common_bits = i & j;
            if (__popc(common_bits & neg_mask) % 2 != 0) {{
                sign *= -1.0f;
            }}
            return sign;
        }}

        // REPORT SECTION 4.3.2: GEOMETRIC COMMUTATOR USING SHARED MEMORY
        __device__ void geometric_product_commutator_device(
            const cfloat* A, const cfloat* B, cfloat* C_out,
            int neg_mask, int n_blades, int dim,
            cfloat* shared_mem_workspace, int block_dim_x
        ) {{
            cfloat* AB = shared_mem_workspace;
            cfloat* BA = &shared_mem_workspace[n_blades];

            for(int i = threadIdx.x; i < n_blades; i += block_dim_x) {{ AB[i] = make_float2(0.0f, 0.0f); BA[i] = make_float2(0.0f, 0.0f); }}
            __syncthreads();

            for (int i = 0; i < n_blades; ++i) {{
                if (A[i].x == 0.0f && A[i].y == 0.0f) continue;
                for (int j = threadIdx.x; j < n_blades; j += block_dim_x) {{
                    if (B[j].x == 0.0f && B[j].y == 0.0f) continue;
                    
                    int k = i ^ j;
                    
                    float sign_ab = calculate_sign_on_the_fly(i, j, neg_mask, dim);
                    cfloat prod_ab;
                    prod_ab.x = (A[i].x * B[j].x - A[i].y * B[j].y) * sign_ab;
                    prod_ab.y = (A[i].x * B[j].y + A[i].y * B[j].x) * sign_ab;
                    atomicAdd(&AB[k].x, prod_ab.x);
                    atomicAdd(&AB[k].y, prod_ab.y);

                    float sign_ba = calculate_sign_on_the_fly(j, i, neg_mask, dim);
                    cfloat prod_ba;
                    prod_ba.x = (B[j].x * A[i].x - B[j].y * A[i].y) * sign_ba;
                    prod_ba.y = (B[j].x * A[i].y + B[j].y * A[i].x) * sign_ba;
                    atomicAdd(&BA[k].x, prod_ba.x);
                    atomicAdd(&BA[k].y, prod_ba.y);
                }}
            }}
            __syncthreads();

            for(int k = threadIdx.x; k < n_blades; k += block_dim_x) {{
                C_out[k].x = AB[k].x - BA[k].x;
                C_out[k].y = AB[k].y - BA[k].y;
            }}
        }}


        extern "C" {{

        // =================================================================================
        // SECTION B: YOUR ORIGINAL KERNELS (UNABRIDGED)
        // =================================================================================
        
        __global__ void gp_kernel(const cfloat* A, const cfloat* B, cfloat* C, int neg_mask, int N_pixels) {{
            const int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx >= N_pixels) return;
            const int base_offset = idx * {n_blades};
            for (int k = 0; k < {n_blades}; ++k) {{
                cfloat result = make_float2(0.0f, 0.0f);
                for (int i = 0; i < {n_blades}; ++i) {{
                    for (int j = 0; j < {n_blades}; ++j) {{
                        if ((i ^ j) == k) {{
                            cfloat a_val = A[base_offset + i];
                            cfloat b_val = B[base_offset + j];
                            if (a_val.x == 0.f && a_val.y == 0.f) continue;
                            if (b_val.x == 0.f && b_val.y == 0.f) continue;
                            int common_bits = i & j;
                            int a_swaps = 0;
                            for (int bit = 0; bit < {dim}; ++bit) {{
                                if ((j >> bit) & 1) {{
                                    a_swaps += countSetBits(i & ~((1 << bit) - 1));
                                }}
                            }}
                            float sign = ((a_swaps % 2) == 0) ? 1.0f : -1.0f;
                             if (__popc(common_bits & neg_mask) % 2 != 0) {{
                                sign *= -1.0f;
                            }}
                            cfloat product;
                            product.x = (a_val.x * b_val.x - a_val.y * b_val.y) * sign;
                            product.y = (a_val.x * b_val.y + a_val.y * b_val.x) * sign;
                            result.x += product.x;
                            result.y += product.y;
                        }}
                    }}
                }}
                C[base_offset + k] = result;
            }}
        }}
        
        __global__ __launch_bounds__(512, 2)
        void calculate_force_kernel(
            const cfloat* __restrict__ psi, const cfloat* __restrict__ mean_psi,
            cfloat* __restrict__ force_out, const int size,
            const float lambda_d, const float* __restrict__ lambda_p_per_grade,
            const float* __restrict__ lambda_b_per_grade, const int* __restrict__ grade_lookup_table,
            const float lambda_p_global, const float lambda_tension, const float lambda_complexity,
            const float lambda_resonance
        ) {{
            extern __shared__ cfloat tile[];
            cfloat* psi_c = &tile[0];
            cfloat* psi_l = &tile[{n_blades} * 1]; cfloat* psi_r = &tile[{n_blades} * 2];
            cfloat* psi_d = &tile[{n_blades} * 3]; cfloat* psi_u = &tile[{n_blades} * 4];
            float* grade_norms_local = (float*)&tile[{n_blades} * 5];
            const int k = threadIdx.x;
            const int x = blockIdx.x;
            const int y = blockIdx.y;
            const int flat_idx_c = (y * size + x) * {n_blades};
            
            psi_c[k] = psi[flat_idx_c + k];
            psi_l[k] = psi[(y * size + (x - 1 + size) % size) * {n_blades} + k];
            psi_r[k] = psi[(y * size + (x + 1) % size) * {n_blades} + k];
            psi_d[k] = psi[(((y - 1 + size) % size) * size + x) * {n_blades} + k];
            psi_u[k] = psi[(((y + 1) % size) * size + x) * {n_blades} + k];
            __syncthreads();
            
            const int grade = grade_lookup_table[k];
            const float grade_specific_lambda_p = lambda_p_per_grade[grade];
            const float effective_lambda_p = grade_specific_lambda_p + lambda_p_global;
            const float lambda_b = lambda_b_per_grade[grade];
            
            cfloat laplacian_k;
            laplacian_k.x = psi_l[k].x + psi_r[k].x + psi_d[k].x + psi_u[k].x - 4.0f * psi_c[k].x;
            laplacian_k.y = psi_l[k].y + psi_r[k].y + psi_d[k].y + psi_u[k].y - 4.0f * psi_c[k].y;
            
            cfloat internal_force_k;
            internal_force_k.x = (lambda_d * laplacian_k.x) - (lambda_b * (psi_c[k].x - mean_psi[k].x)) - (effective_lambda_p * psi_c[k].x);
            internal_force_k.y = (lambda_d * laplacian_k.y) - (lambda_b * (psi_c[k].y - mean_psi[k].y)) - (effective_lambda_p * psi_c[k].y);
            
            cfloat total_force_k = internal_force_k;
            
            if (lambda_tension > 1e-9f) {{
                if (k < {dim + 1}) {{
                    grade_norms_local[k] = 0.0f;
                }}
                __syncthreads();
                float norm_sq_k = psi_c[k].x * psi_c[k].x + psi_c[k].y * psi_c[k].y;
                atomicAdd(&grade_norms_local[grade], norm_sq_k);
                __syncthreads();
                float shannon_entropy;
                if (k == 0) {{
                    float total_norm_point = 0.0f;
                    for (int g = 0; g <= {dim}; ++g) {{
                        total_norm_point += grade_norms_local[g];
                    }}
                    float current_entropy = 0.0f;
                    if (total_norm_point > 1e-9f) {{
                        for (int g = 0; g <= {dim}; ++g) {{
                            float p_g = grade_norms_local[g] / total_norm_point;
                            if (p_g > 1e-9f) {{
                                current_entropy -= p_g * logf(p_g);
                            }}
                        }}
                    }}
                    shannon_entropy = current_entropy / {max_entropy_val}f;
                    grade_norms_local[{dim+1}] = shannon_entropy;
                }}
                __syncthreads();
                
                shannon_entropy = grade_norms_local[{dim+1}];
                float simplicity_penalty = 1.0f - shannon_entropy;
                total_force_k.x += lambda_tension * simplicity_penalty * psi_c[k].x;
                total_force_k.y += lambda_tension * simplicity_penalty * psi_c[k].y;
                if (lambda_resonance > 1e-9f) {{
                    float norm_sq_point = 0.0f;
                    for(int g=0; g <= {dim}; ++g) {{
                        norm_sq_point += grade_norms_local[g];
                    }}
                    total_force_k.x += lambda_resonance * norm_sq_point * psi_c[k].x;
                    total_force_k.y += lambda_resonance * norm_sq_point * psi_c[k].y;
                }}
            }}
            if (lambda_complexity > 1e-9f) {{
                float laplacian_norm_sq = laplacian_k.x * laplacian_k.x + laplacian_k.y * laplacian_k.y;
                float complexity_penalty = 1.0f / (1.0f + 2.0f * laplacian_norm_sq);
                total_force_k.x += lambda_complexity * complexity_penalty * psi_c[k].x;
                total_force_k.y += lambda_complexity * complexity_penalty * psi_c[k].y;
            }}
            force_out[flat_idx_c + k] = total_force_k;
        }}

        __global__ __launch_bounds__(256, 4)
        void update_state_kernel( cfloat* __restrict__ psi, cfloat* __restrict__ pi, const cfloat* __restrict__ force, const float* __restrict__ dt_ptr, const float damping, const int n_elements, const float thermal_noise_level, unsigned int* rand_states) {{
            const int idx = blockIdx.x * blockDim.x + threadIdx.x;
            if (idx < n_elements) {{
                const float dt = *dt_ptr;
                cfloat pi_k = pi[idx];
                cfloat force_k = force[idx];
                if (thermal_noise_level > 0.0f) {{
                    unsigned int localState = rand_states[idx];
                    float rand_x = lcg_uniform(localState);
                    float rand_y = lcg_uniform(localState);
                    rand_states[idx] = localState;
                    cfloat noise_force;
                    noise_force.x = (rand_x - 0.5f) * 2.0f;
                    noise_force.y = (rand_y - 0.5f) * 2.0f;
                    force_k.x += thermal_noise_level * noise_force.x;
                    force_k.y += thermal_noise_level * noise_force.y;
                }}
                pi_k.x += dt * (force_k.x - damping * pi_k.x);
                pi_k.y += dt * (force_k.y - damping * pi_k.y);
                psi[idx].x += dt * pi_k.x;
                psi[idx].y += dt * pi_k.y;
                pi[idx] = pi_k;
            }}
        }}

        __global__ __launch_bounds__(256, 4) void sample_partial_reduction_kernel(const cfloat* __restrict__ psi, const int* __restrict__ grades, float* __restrict__ partial_sums_out, int total_elements) {{ extern __shared__ float s_data[]; const int tid = threadIdx.x; const int blockId = blockIdx.x; const int gridSize = gridDim.x; if (tid < {dim + 1}) {{ s_data[tid] = 0.0f; }} __syncthreads(); for (int i = blockId * blockDim.x + tid; i < total_elements; i += blockDim.x * gridSize) {{ const int blade_idx = i % {n_blades}; const int grade = grades[blade_idx]; const float real_amplitude = psi[i].x; atomicAdd(&s_data[grade], real_amplitude); }} __syncthreads(); if (tid < {dim + 1}) {{ partial_sums_out[blockId * {dim + 1} + tid] = s_data[tid]; }} }}
        __global__ __launch_bounds__(32, 16) void sample_final_reduction_kernel( const float* __restrict__ partial_sums_in, float* __restrict__ time_series_out, int num_blocks, int sample_index, int num_samples) {{ const int grade_idx = blockIdx.x * blockDim.x + threadIdx.x; if (grade_idx >= {dim + 1}) return; float total_sum = 0.0f; for (int i = 0; i < num_blocks; ++i) {{ total_sum += partial_sums_in[i * {dim + 1} + grade_idx]; }} time_series_out[grade_idx * num_samples + sample_index] = total_sum; }}
        __global__ __launch_bounds__(512, 2) void force_and_reduce_kernel( const cfloat* __restrict__ psi, const cfloat* __restrict__ pi, cfloat* __restrict__ force_out, float* partial_max_vals_out, const int size, const float lambda_d, const float* __restrict__ lambda_p_per_grade, const float* __restrict__ lambda_b_per_grade, const int* __restrict__ grade_lookup_table, const float lambda_p_global) {{ extern __shared__ cfloat tile[]; float* reduction_tile = (float*)&tile[{n_blades} * 5]; cfloat* psi_c = &tile[0]; cfloat* psi_l = &tile[{n_blades}]; cfloat* psi_r = &tile[{n_blades} * 2]; cfloat* psi_d = &tile[{n_blades} * 3]; cfloat* psi_u = &tile[{n_blades} * 4]; const int k = threadIdx.x; const int block_x = blockIdx.x; const int block_y = blockIdx.y; const int grid_w = gridDim.x; const int block_flat_idx = block_y * grid_w + block_x; const int flat_idx_c = (block_y * size + block_x) * {n_blades}; psi_c[k] = psi[flat_idx_c + k]; psi_l[k] = psi[(block_y * size + (block_x - 1 + size) % size) * {n_blades} + k]; psi_r[k] = psi[(block_y * size + (block_x + 1) % size) * {n_blades} + k]; psi_d[k] = psi[(((block_y - 1 + size) % size) * size + block_x) * {n_blades} + k]; psi_u[k] = psi[(((block_y + 1) % size) * size + block_x) * {n_blades} + k]; __syncthreads(); const int grade = grade_lookup_table[k]; const float effective_lambda_p = lambda_p_per_grade[grade] + lambda_p_global; cfloat laplacian_k; laplacian_k.x = psi_l[k].x + psi_r[k].x + psi_d[k].x + psi_u[k].x - 4.0f * psi_c[k].x; laplacian_k.y = psi_l[k].y + psi_r[k].y + psi_d[k].y + psi_u[k].y - 4.0f * psi_c[k].y; cfloat force_k; force_k.x = (lambda_d * laplacian_k.x) - (effective_lambda_p * psi_c[k].x); force_k.y = (lambda_d * laplacian_k.y) - (effective_lambda_p * psi_c[k].y); force_out[flat_idx_c + k] = force_k; cfloat pi_k = pi[flat_idx_c + k]; float mag_pi = hypotf(pi_k.x, pi_k.y); float mag_force = hypotf(force_k.x, force_k.y); reduction_tile[k] = mag_pi; reduction_tile[{n_blades} + k] = mag_force; __syncthreads(); for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {{ if (k < s) {{ reduction_tile[k] = fmaxf(reduction_tile[k], reduction_tile[k + s]); reduction_tile[{n_blades} + k] = fmaxf(reduction_tile[{n_blades} + k], reduction_tile[{n_blades} + k + s]); }} __syncthreads(); }} if (k == 0) {{ partial_max_vals_out[block_flat_idx * 2 + 0] = reduction_tile[0]; partial_max_vals_out[block_flat_idx * 2 + 1] = reduction_tile[{n_blades} + 0]; }} }}
        __global__ void finalize_reduction_and_calc_dt_kernel( const float* partial_max_vals_in, float* dt_out, int num_blocks, float cfl_v_safety, float cfl_f_safety, float dt_ceiling, float epsilon) {{ float max_v_global = 0.0f; float max_f_global = 0.0f; for (int i = threadIdx.x; i < num_blocks; i += blockDim.x) {{ max_v_global = fmaxf(max_v_global, partial_max_vals_in[i * 2 + 0]); max_f_global = fmaxf(max_f_global, partial_max_vals_in[i * 2 + 1]); }} extern __shared__ float final_reduction_tile[]; final_reduction_tile[threadIdx.x] = max_v_global; final_reduction_tile[blockDim.x + threadIdx.x] = max_f_global; __syncthreads(); for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {{ if (threadIdx.x < s) {{ final_reduction_tile[threadIdx.x] = fmaxf(final_reduction_tile[threadIdx.x], final_reduction_tile[threadIdx.x + s]); final_reduction_tile[blockDim.x + threadIdx.x] = fmaxf(final_reduction_tile[blockDim.x + threadIdx.x], final_reduction_tile[blockDim.x + threadIdx.x + s]); }} __syncthreads(); }} if (threadIdx.x == 0) {{ max_v_global = final_reduction_tile[0]; max_f_global = final_reduction_tile[blockDim.x]; float dt_from_velocity = cfl_v_safety / (max_v_global + epsilon); float dt_from_force = cfl_f_safety / (sqrtf(max_f_global) + epsilon); float dt = fminf(dt_from_velocity, dt_from_force); dt = fminf(dt, dt_ceiling); dt_out[0] = dt; }} }}
        __global__ __launch_bounds__(256, 4) void update_state_with_gpu_dt_kernel( cfloat* __restrict__ psi, cfloat* __restrict__ pi, const cfloat* __restrict__ force, const float* __restrict__ dt_gpu_buffer, const float damping, const int n_elements, float* current_time_gpu) {{ const int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx == 0) {{ current_time_gpu[0] += dt_gpu_buffer[0]; }} if (idx < n_elements) {{ const float dt = dt_gpu_buffer[0]; cfloat pi_k = pi[idx]; cfloat force_k = force[idx]; pi_k.x += dt * (force_k.x - damping * pi_k.x); pi_k.y += dt * (force_k.y - damping * pi_k.y); psi[idx].x += dt * pi_k.x; psi[idx].y += dt * pi_k.y; pi[idx] = pi_k; }} }}

        __global__ __launch_bounds__(512, 2)
        void evolve_and_sample_fused_kernel(
            cfloat* __restrict__ psi, cfloat* __restrict__ pi,
            float* __restrict__ time_series_out, const int size,
            const int total_steps_to_run, const int sample_interval, const int num_samples,
            const float* __restrict__ lambda_p_per_grade, const int* __restrict__ grade_lookup_table,
            const float lambda_d, const float lambda_p_global, const float damping_evolve,
            const float initial_dt
        ) {{
            extern __shared__ cfloat tile[];
            cfloat* psi_l_tile = &tile[0];
            cfloat* psi_r_tile = &tile[{n_blades}];
            cfloat* psi_d_tile = &tile[{n_blades} * 2];
            cfloat* psi_u_tile = &tile[{n_blades} * 3];
            
            const int k = threadIdx.x;
            const int x = blockIdx.x;
            const int y = blockIdx.y;
            const int flat_idx_c = (y * size + x) * {n_blades} + k;
            cfloat psi_k_reg = psi[flat_idx_c];
            cfloat pi_k_reg  = pi[flat_idx_c];
            
            for (int t = 0; t < total_steps_to_run; ++t) {{
                psi_l_tile[k] = psi[(y * size + (x - 1 + size) % size) * {n_blades} + k];
                psi_r_tile[k] = psi[(y * size + (x + 1) % size) * {n_blades} + k];
                psi_d_tile[k] = psi[(((y - 1 + size) % size) * size + x) * {n_blades} + k];
                psi_u_tile[k] = psi[(((y + 1) % size) * size + x) * {n_blades} + k];
                __syncthreads();
                cfloat laplacian_k;
                laplacian_k.x = psi_l_tile[k].x + psi_r_tile[k].x + psi_d_tile[k].x + psi_u_tile[k].x - 4.0f * psi_k_reg.x;
                laplacian_k.y = psi_l_tile[k].y + psi_r_tile[k].y + psi_d_tile[k].y + psi_u_tile[k].y - 4.0f * psi_k_reg.y;
                const int grade = grade_lookup_table[k];
                const float effective_lambda_p = lambda_p_per_grade[grade] + lambda_p_global;
                cfloat force_k;
                force_k.x = (lambda_d * laplacian_k.x) - (effective_lambda_p * psi_k_reg.x);
                force_k.y = (lambda_d * laplacian_k.y) - (effective_lambda_p * psi_k_reg.y);
                pi_k_reg.x += initial_dt * (force_k.x - damping_evolve * pi_k_reg.x);
                pi_k_reg.y += initial_dt * (force_k.y - damping_evolve * pi_k_reg.y);
                psi_k_reg.x += initial_dt * pi_k_reg.x;
                psi_k_reg.y += initial_dt * pi_k_reg.y;
                psi[flat_idx_c] = psi_k_reg;
                pi[flat_idx_c]  = pi_k_reg;
                if (sample_interval > 0 && (t + 1) % sample_interval == 0) {{
                    int sample_idx = (t + 1) / sample_interval - 1;
                    if (sample_idx < num_samples) {{
                        atomicAdd(&time_series_out[grade * num_samples + sample_idx], psi_k_reg.x);
                    }}
                }}
                __syncthreads(); 
            }}
        }}
        
        __global__ void feynman_sample_kernel(
            const cfloat* __restrict__ psi, const int* __restrict__ bivector_indices,
            float* __restrict__ time_series_out, int num_bivectors,
            int sample_index, int num_samples, int size
        ) {{
            const int bivector_loop_idx = threadIdx.x;
            if (bivector_loop_idx >= num_bivectors) return;
            const int blade_idx_to_sample = bivector_indices[bivector_loop_idx];
            float total_amplitude = 0.0f;
            for (int i = 0; i < size * size; ++i) {{
                total_amplitude += psi[i * {n_blades} + blade_idx_to_sample].x;
            }}
            time_series_out[bivector_loop_idx * num_samples + sample_index] = total_amplitude / (float)(size * size);
        }}


        // =================================================================================
        // SECTION C: NEW, CORRECTED, AND OPTIMIZED FEYNMAN KERNEL
        // =================================================================================
        __global__ __launch_bounds__(256, 4) // Launch bounds set
        void evolve_feynman_fused_kernel(
            cfloat* __restrict__ psi, cfloat* __restrict__ pi,
            float* __restrict__ bivector_time_series_out, const int size,
            const int total_steps_to_run, const int sample_interval, const int num_samples,
            const float* __restrict__ lambda_p_per_grade, const int* __restrict__ grade_lookup_table,
            const float lambda_d, const float lambda_p_global, const float damping,
            const float initial_dt,
            const cfloat* __restrict__ b_field_global, const float b_field_strength,
            const int* __restrict__ bivector_indices, const int num_bivectors,
            const int neg_mask, const int algebra_dim
        ) {{
            extern __shared__ cfloat tile[];
            
            // Shared Memory Layout
            cfloat* psi_l_tile = &tile[0];
            cfloat* psi_r_tile = &tile[{n_blades}];
            cfloat* psi_d_tile = &tile[{n_blades} * 2];
            cfloat* psi_u_tile = &tile[{n_blades} * 3];
            cfloat* psi_c_tile         = &tile[{n_blades} * 4]; // <<< CORRECTION: Space for the full psi_c
            cfloat* torque_force_local = &tile[{n_blades} * 5];
            cfloat* comm_workspace     = &tile[{n_blades} * 6]; // 2*n_blades space for the commutator
        
            const int k = threadIdx.x;
            const int x = blockIdx.x;
            const int y = blockIdx.y;
            const int block_dim_x = blockDim.x;
            const int flat_idx_c = (y * size + x) * {n_blades} + k;
        
            cfloat psi_k_reg = psi[flat_idx_c];
            cfloat pi_k_reg  = pi[flat_idx_c];
            
            for (int t = 0; t < total_steps_to_run; ++t) {{
                // Tiling: Load neighbors into shared memory
                psi_l_tile[k] = psi[(y * size + (x - 1 + size) % size) * {n_blades} + k];
                psi_r_tile[k] = psi[(y * size + (x + 1) % size) * {n_blades} + k];
                psi_d_tile[k] = psi[(((y - 1 + size) % size) * size + x) * {n_blades} + k];
                psi_u_tile[k] = psi[(((y + 1) % size) * size + x) * {n_blades} + k];
                
                // <<< PHYSICAL ERROR CORRECTION: Load the entire psi into shared memory >>>
                psi_c_tile[k] = psi_k_reg;
                __syncthreads();
        
                // 1. Internal Forces (Laplacian, etc.)
                cfloat laplacian_k;
                laplacian_k.x = psi_l_tile[k].x + psi_r_tile[k].x + psi_d_tile[k].x + psi_u_tile[k].x - 4.0f * psi_k_reg.x;
                laplacian_k.y = psi_l_tile[k].y + psi_r_tile[k].y + psi_d_tile[k].y + psi_u_tile[k].y - 4.0f * psi_k_reg.y;
                
                const int grade = grade_lookup_table[k];
                const float effective_lambda_p = lambda_p_per_grade[grade] + lambda_p_global;
        
                cfloat internal_force_k;
                internal_force_k.x = (lambda_d * laplacian_k.x) - (effective_lambda_p * psi_k_reg.x);
                internal_force_k.y = (lambda_d * laplacian_k.y) - (effective_lambda_p * psi_k_reg.y);
        
                // 2. Magnetic Torque Force (CORRECT CALCULATION)
                // The full psi_c_tile is now used instead of psi_B_local.
                geometric_product_commutator_device(
                    psi_c_tile, b_field_global, torque_force_local,
                    neg_mask, {n_blades}, algebra_dim,
                    comm_workspace, block_dim_x
                );
                __syncthreads();
        
                
                // 3. Total Force
                cfloat total_force_k; // = internal_force_k; // <-- COMMENT OUT THIS LINE
                
                // Let's keep only the magnetic force:
                total_force_k.x = b_field_strength * torque_force_local[k].x;
                total_force_k.y = b_field_strength * torque_force_local[k].y;
                // total_force_k.x += b_field_strength * torque_force_local[k].x; // <-- COMMENT OUT THIS LINE
                // total_force_k.y += b_field_strength * torque_force_local[k].y; // <-- COMMENT OUT THIS LINE
                
                // 4. State Update
                // Let's also disable damping
                pi_k_reg.x += initial_dt * total_force_k.x; // * (total_force_k.x - damping * pi_k_reg.x);
                pi_k_reg.y += initial_dt * total_force_k.y; // * (total_force_k.y - damping * pi_k_reg.y);

                psi_k_reg.x += initial_dt * pi_k_reg.x;
                psi_k_reg.y += initial_dt * pi_k_reg.y;
        
                psi[flat_idx_c] = psi_k_reg;
                pi[flat_idx_c]  = pi_k_reg;
        
                // 5. Sampling
                 if (sample_interval > 0 && (t + 1) % sample_interval == 0) {{
                     int sample_idx = (t + 1) / sample_interval - 1;
                     if (sample_idx < num_samples) {{
                         // ### CORRECTION: Save each bivector to its own row ###
                         // We use a loop to find which bivector it is.
                        for(int i = 0; i < num_bivectors; ++i) {{
                             if(k == bivector_indices[i]) {{
                                 // i: 0-based bivector index (0th, 1st, 2nd bivector...)
                                 // This places the signal in the correct row.
                                 atomicAdd(&bivector_time_series_out[i * num_samples + sample_idx], psi_k_reg.x);
                             }}
                        }}
                     }}
                 }}
                 __syncthreads();
               }}
            }}

        }} // end extern "C"
        '''
        
        # --- BINDING ALL KERNELS TO PYTHON ---
        self.module = cp.RawModule(code=cuda_code, options=('--std=c++11',))
        
        # get_function calls for all kernels
        self.gp_kernel = self.module.get_function("gp_kernel")
        self.calculate_force_kernel = self.module.get_function("calculate_force_kernel")
        self.update_state_kernel = self.module.get_function("update_state_kernel")
        self.sample_partial_kernel = self.module.get_function("sample_partial_reduction_kernel")
        self.sample_final_kernel = self.module.get_function("sample_final_reduction_kernel")
        self.force_and_reduce_kernel = self.module.get_function("force_and_reduce_kernel")
        self.finalize_reduction_and_calc_dt_kernel = self.module.get_function("finalize_reduction_and_calc_dt_kernel")
        self.update_state_with_gpu_dt_kernel = self.module.get_function("update_state_with_gpu_dt_kernel")
        self.evolve_and_sample_fused_kernel = self.module.get_function("evolve_and_sample_fused_kernel")
        self.feynman_sample_kernel = self.module.get_function("feynman_sample_kernel")
        self.evolve_feynman_fused_kernel = self.module.get_function("evolve_feynman_fused_kernel")
        
        self.magnitude_max_kernel = cp.ReductionKernel('T x', 'float32 y', 'hypotf(x.real(), x.imag())', 'fmaxf(a, b)','y = a', '0.0f', 'magnitude_max_reduce')

    def _gp(self, A, B, out):
        N_pixels = A.shape[0] * A.shape[1]
        block_size = 256
        if A.size > 0: self.gp_kernel((N_pixels,), (block_size,), (A, B, out, self.alg.neg_metric_mask.item(), N_pixels))

    def _calculate_force(self):
        mean_psi = cp.mean(self.psi, axis=(0, 1), keepdims=False)
        grid_size = (self.size, self.size)
        block_size = (self.alg.num_blades,)
        shared_mem_size = (5 * self.alg.num_blades * 8) + ((self.alg.dim + 2) * 4)
        
        self.calculate_force_kernel(
            grid_size, block_size,
            (self.psi, mean_psi, self.force_buffer, self.size, self.lambda_d, self.lambda_p_gpu, self.lambda_b_gpu, self.alg.grades, self.lambda_p_global, self.lambda_tension, self.lambda_complexity, self.lambda_resonance),
            shared_mem=shared_mem_size
        )
        return self.force_buffer
        
    def evolve_optimized(self, initial_psi, probe_multivector, total_evolution_time, num_samples, quiet=False, pbar=None):
        with self.stream:
            self.psi = initial_psi.copy()
            self.pi.fill(0)
            
            # Apply initial perturbation
            probe_amplitude = 2.0
            probe_width_factor = 0.50
            coords = cp.arange(self.size, dtype=cp.float32)
            x, y = cp.meshgrid(coords, coords)
            center, width = self.size / 2.0, self.size * probe_width_factor
            probe_shape = (probe_amplitude * cp.exp(-((x - center)**2 + (y - center)**2) / (2 * width**2))).astype(cp.complex64)
            self.pi += probe_shape.reshape(self.size, self.size, 1) * probe_multivector

            # Smart initial time step (dt) calculation
            CFL_VELOCITY_SAFETY, CFL_FORCE_SAFETY, EPSILON = 0.15, 0.15, 1e-9
            DT_STRUCTURAL_CEILING = 0.2 / (float(self.lambda_d) + EPSILON)
            
            force_initial = self._calculate_force()
            max_v = cp.max(cp.abs(self.pi)).get().item()
            max_f = cp.max(cp.abs(force_initial)).get().item()
            dt_from_v = CFL_VELOCITY_SAFETY / (max_v + EPSILON) if max_v > 0 else float('inf')
            dt_from_f = CFL_FORCE_SAFETY / (math.sqrt(max_f) + EPSILON) if max_f > 0 else float('inf')
            initial_dt = min(dt_from_v, dt_from_f, DT_STRUCTURAL_CEILING, 1.0)

            if initial_dt <= 0 or not math.isfinite(initial_dt):
                if pbar: pbar.write("WARNING: Invalid initial dt. Skipping evolution.")
                return {}, -1

            total_steps_to_run = int(total_evolution_time / initial_dt)
            if total_steps_to_run <= 0:
                if pbar: pbar.write("WARNING: Total number of steps is zero. Skipping evolution.")
                return {}, -1

            # PBAR MANAGEMENT CORRECTION (TAKEN FROM PROMETHEUS)
            if pbar:
                pbar.reset(total=total_steps_to_run)
                pbar.set_description(f"Listening ({self.size}x{self.size}) [Step: {total_steps_to_run}]")

            sample_interval_steps = max(1, total_steps_to_run // num_samples)
            if sample_interval_steps == 0: sample_interval_steps = 1 # Important safety measure
            
            time_series_gpu = cp.zeros((self.alg.dim + 1, num_samples), dtype=cp.float32)
            
            grid_size = (self.size, self.size)
            block_size = (self.alg.num_blades,)
            shared_mem_size = 4 * self.alg.num_blades * 8 

            self.evolve_and_sample_fused_kernel(
                grid_size, block_size,
                args=(self.psi, self.pi, time_series_gpu, self.size, total_steps_to_run, 
                      sample_interval_steps, num_samples, self.lambda_p_gpu, self.alg.grades, 
                      self.lambda_d, self.lambda_p_global, self.damping_evolve, cp.float32(initial_dt)),
                shared_mem=shared_mem_size
            )
            
            self.stream.synchronize()

            # PBAR MANAGEMENT CORRECTION (TAKEN FROM PROMETHEUS)
            if pbar:
                pbar.n = total_steps_to_run
                pbar.refresh()

            pixels_count = self.size * self.size
            time_series_gpu /= pixels_count
            
            output_dict = {'all_grades': time_series_gpu}

            # === SOLUTION IS HERE ===
            # Calculate the actual time difference between samples, required for analysis.
            true_dt_per_sample = total_evolution_time / num_samples if num_samples > 0 else 0
            
            # Now returning the correct 'true_dt_per_sample' instead of the erroneous 'initial_dt'.
            return output_dict, true_dt_per_sample


    
    
    
  
    def relax_to_plateau(self, max_steps, quiet=False, pbar=None):
        """
        FINAL, STABLE RELAXATION METHOD (v6.8 - Smart Controller)
        - 'Dead zone' and 'heating limit' added to the controller without touching P_GAIN.
        - Prevents overreactions caused by floating-point precision issues.
        """
        # --- Parameters and Criteria ---
        CFL_SAFETY = 1e-9
        FINAL_DAMPING = self.params.get('damping_relax', 1.382242)
        CHECK_INTERVAL = 250
        
        THE_TARGET_KE = 1.0e-10
        TOLERANCE = 0.01
        PLATEAU_TARGET_MIN = THE_TARGET_KE * (1.0 - TOLERANCE)
        PLATEAU_TARGET_MAX = THE_TARGET_KE * (1.0 + TOLERANCE)
        
        # --- NEW CONTROLLER PARAMETERS ---
        PROPORTIONAL_GAIN = 1.0       # As requested, we are not touching this parameter.
        CONTROLLER_DEAD_ZONE_LOG = 0.01 # If the log(KE) error is smaller than this value, do nothing (sensitivity threshold).
        MAX_HEATING_PULSE = 0.3         # Ceiling value for the heating pulse even if the dead zone is exceeded.
        MAX_DAMPING_FACTOR = 5.0        # Ceiling value for the cooling factor (remains the same).
        # ------------------------------------
        
        PATIENCE = 150
        MIN_KE_THRESHOLD = 1e-13
        MIN_VOLATILITY_THRESHOLD = 1e-5
    
        SLOPE_TIME_SCALE_THRESHOLD = 5.0e2 
        SLOPE_VOLATILITY_THRESHOLD = 1e-9
        RELATIVE_CHANGE_THRESHOLD = 0.005
        
        LONG_TERM_HISTORY_SIZE = 150 
        LONG_TERM_DRIFT_THRESHOLD = 0.005 
    
        short_term_history_len = 150
        energy_history, mean_energy_history, slope_history = [], [], []
        long_term_mean_energy_history = []
        
        noise_buffer = (cp.random.randn(*self.psi.shape, dtype=DTYPE_FLOAT) + 
                        1j * cp.random.randn(*self.psi.shape, dtype=DTYPE_FLOAT))
    
        with self.stream:
            if cp.all(self.psi == 0):
                self.psi = (cp.random.randn(self.size, self.size, self.alg.num_blades, dtype=DTYPE_FLOAT) + 1j * cp.random.randn(self.size, self.size, self.alg.num_blades, dtype=DTYPE_FLOAT)) * 1e-7
            self.pi.fill(0)
            
            if pbar: pbar.reset(total=max_steps)
    
            current_damping = FINAL_DAMPING
            convergence_counter = 0
            temp_damping = current_damping
    
            for t in range(1, max_steps + 1):
                if pbar: pbar.update(1)
    
                force = self._calculate_force()
                if not cp.all(cp.isfinite(force)):
                    if pbar: pbar.write(f"❌ Explosion! Step: {t}.")
                    return None
                
                max_v, max_f = cp.max(cp.abs(self.pi)), cp.max(cp.abs(force))
                dt_val = CFL_SAFETY / (1.0 + float(max_v) + float(max_f))
                dt = float(np.clip(dt_val, a_min=1e-10, a_max=0.01))
    
                if t % CHECK_INTERVAL == 0:
                    ke = cp.sum(cp.real(self.pi * cp.conj(self.pi))).get().item()
                    energy_history.append(ke)
                    if len(energy_history) > short_term_history_len: energy_history.pop(0)
    
                    if len(energy_history) == short_term_history_len:
                        mean_energy = np.mean(energy_history)
                        
                        # --- UPDATED CONTROLLER LOGIC ---
                        log_error = np.log10(mean_energy + 1e-30) - np.log10(THE_TARGET_KE)
                        
                        # 'Dead Zone' control
                        if abs(log_error) > CONTROLLER_DEAD_ZONE_LOG:
                            if log_error < 0: # Energy is too low, heating is required
                                # Calculate the heating factor
                                heating_factor = abs(log_error) * PROPORTIONAL_GAIN
                                # Limit the heating pulse to prevent sudden spikes
                                heating_factor = min(heating_factor, MAX_HEATING_PULSE) 
                                
                                final_noise_strength = cp.sqrt(dt) * self.base_noise_level * heating_factor
                                self.pi += noise_buffer * final_noise_strength
                                temp_damping = current_damping
                            else: # Energy is too high, cooling is required
                                cooling_factor = 1.0 + (log_error * PROPORTIONAL_GAIN)
                                cooling_factor = min(cooling_factor, MAX_DAMPING_FACTOR)
                                temp_damping = current_damping * cooling_factor
                        else:
                            # Error is within the 'dead zone', leave the system alone.
                            temp_damping = current_damping
                        # --- END OF UPDATE ---
                        
                        is_alive = mean_energy > MIN_KE_THRESHOLD
                        # ... (rest of the code is the same)
                        is_vibrating = (np.std(energy_history) / mean_energy if mean_energy > 1e-9 else 1.0) > MIN_VOLATILITY_THRESHOLD
                        
                        mean_energy_history.append(mean_energy)
                        if len(mean_energy_history) > short_term_history_len: mean_energy_history.pop(0)
                        
                        mean_slope, has_stable_slope = np.nan, False
                        is_glacially_slow = False
    
                        if len(mean_energy_history) == short_term_history_len:
                            mean_slope, _, _, _, _ = linregress(np.arange(short_term_history_len), mean_energy_history)
                            
                            if not np.isnan(mean_slope) and abs(mean_slope) > 1e-30:
                                characteristic_time = (mean_energy * 0.01) / abs(mean_slope)
                                is_glacially_slow = characteristic_time > SLOPE_TIME_SCALE_THRESHOLD
                            else:
                                is_glacially_slow = True
    
                            if not np.isnan(mean_slope): slope_history.append(mean_slope)
                            if len(slope_history) > short_term_history_len: slope_history.pop(0)
                            if len(slope_history) == short_term_history_len:
                                has_stable_slope = np.std(slope_history) < SLOPE_VOLATILITY_THRESHOLD
                                
                        is_truly_static = ((np.max(energy_history) - np.min(energy_history)) / (mean_energy + 1e-20)) < RELATIVE_CHANGE_THRESHOLD
    
                        has_no_long_term_drift = False
                        long_term_mean_energy_history.append(mean_energy)
                        if len(long_term_mean_energy_history) > LONG_TERM_HISTORY_SIZE: long_term_mean_energy_history.pop(0)
                        if len(long_term_mean_energy_history) == LONG_TERM_HISTORY_SIZE:
                            drift = abs(mean_energy - long_term_mean_energy_history[0]) / (mean_energy + 1e-20)
                            has_no_long_term_drift = drift < LONG_TERM_DRIFT_THRESHOLD
                        
                        is_in_target_zone = PLATEAU_TARGET_MIN <= mean_energy <= PLATEAU_TARGET_MAX
    
                        if pbar:
                            s1,s2,s3,s4,s5,s6 = "A✅" if is_alive else "A❌", "V✅" if is_vibrating else "V❌", "S✅" if has_stable_slope else "S❌", "R✅" if is_truly_static else "R❌", "L✅" if has_no_long_term_drift else "L❌", "T✅" if is_in_target_zone else "T❌"
                            s7 = "G✅" if is_glacially_slow else "G❌"
                            status_flags = f"[{s1}{s2}{s3}{s4}{s5}{s6}{s7}]"
                            slope_disp = f"{mean_slope:.1e}" if not np.isnan(mean_slope) else "N/A"
                            pbar.set_description(f"Searching for Plateau{status_flags} [KE:{mean_energy:.2e}|M.Slope:{slope_disp}]")
                        
                        if is_alive and is_vibrating and has_stable_slope and is_truly_static and has_no_long_term_drift and is_in_target_zone and is_glacially_slow:
                            convergence_counter += 1
                        else:
                            convergence_counter = 0
                            
                        if convergence_counter >= PATIENCE:
                            if pbar: pbar.write(f"\n✅ SUCCESS: System met ALL 8 stability and target criteria at step {t}!")
                            self.pi += dt * (force - temp_damping * self.pi)
                            self.psi += dt * self.pi
                            return self.psi
    
                self.pi += dt * (force - temp_damping * self.pi)
                self.psi += dt * self.pi
    
            if pbar: pbar.write(f"\n⚠️ Plateau search reached maximum steps.")
            return self.psi



    
    

# ======================================================================================
# SECTION 2: EXPERIMENT ORCHESTRATOR (ProjectDaVinci Class)
# ======================================================================================
class ProjectDaVinci:
    def __init__(self, champion_params, grid_scales, base_config, alg, results_dir='davinci_results'):
        self.params = champion_params
        self.grid_scales = grid_scales
        self.config = base_config
        self.alg = alg
        self.results_dir = results_dir
        os.makedirs(self.results_dir, exist_ok=True)
        self._GRADE_PREFIX_MAP = {
            0: 'Scalar', 1: 'Vector', 2: 'Bivector', 3: 'Trivector',
            4: 'Quadvector', 5: 'Grade5', 6: 'Grade6', 7: 'Grade7',
            8: 'Grade8', 9: 'Pseudoscalar'
        }
        self.all_results = {size: {} for size in grid_scales}

    def _calculate_and_present_alpha_from_params(self, champion_params):
        """
        Calculates and reports the Fine-Structure Constant (alpha) from the
        model's intrinsic geometry using the champion parameters.
        """
        # Get the required parameters
        lambda_d = champion_params.get('lambda_d')
        lambda_p_g0 = champion_params.get('lambda_p_g0')

        if lambda_d is None or lambda_p_g0 is None:
            print("❌ Required parameters lambda_d or lambda_p_g0 for alpha calculation were not found.")
            return

        # Known physical constant
        ALPHA_REAL = 1 / 137.036
        pi = np.pi

        # Step 1: Calculate the model's core physics ratio
        core_physics_ratio = lambda_p_g0 / lambda_d

        # Step 2: Calculate the theoretical constant from Cl(1,8) geometry
        geometric_constant_theory = (8/9) * 4 * pi

        # Step 3: Calculate the model's precise prediction
        alpha_predicted_precise = core_physics_ratio / geometric_constant_theory
        
        # Step 4: Report the final results and the precise error
        final_error_percent = (abs(alpha_predicted_precise - ALPHA_REAL) / ALPHA_REAL) * 100

        print("\n" + "="*75)
        print("PROOF 3: FUNDAMENTAL CONSTANT DERIVED FROM THE MODEL'S INTRINSIC GEOMETRY")
        print("="*75)
        print("This report shows how the same champion parameters that generate the particle spectrum")
        print("also predict a fundamental constant from the model's own geometric structure.")
        print("\nTheoretical Formula: α = (λ_p_g0 / λ_d) / ( (8/9) * 4π )")
        print("-" * 75)
        print(f"Model's Core Physics Ratio (λ_p_g0 / λ_d): {core_physics_ratio:.8f}")
        print(f"Model's Geometric Constant ( (8/9) * 4π ):  {geometric_constant_theory:.8f}")
        print("-" * 75)
        print(f"Model's Precise Prediction (α_predicted):   {alpha_predicted_precise:.8f}")
        print(f"Actual Value (α_real):                      {ALPHA_REAL:.8f}")
        print(f"Inverse Values: 1/α_predicted = {1/alpha_predicted_precise:.4f}, 1/α_real = {1/ALPHA_REAL:.4f}")
        print(f"\nFINAL ERROR MARGIN: {final_error_percent:.4f}%")
        print("="*75)
        
        return final_error_percent 
        
    def _present_raw_particle_table(self, grid_size, all_particles_at_size, scaling_factor):
        """Analyzes and presents the raw results of a single grid size without extrapolation."""
        print(f"\n--- DIAGNOSTIC REPORT: {grid_size}x{grid_size} RAW SPECTRUM ANALYSIS ---")
        if not all_particles_at_size or scaling_factor is None:
            print("Raw data analysis could not be performed for this grid.")
            return

        MATCH_THRESHOLD_PERCENT = 5.0 # Let's broaden the threshold for diagnostics (5%)
        results = []
        pdg_masses_list = sorted([(name, data['mass']) for name, data in PDG_DATA.items()], key=lambda x: x[1])

        # Find the best match for each found particle
        for p_data in all_particles_at_size:
            predicted_mass = p_data['freq'] * scaling_factor
            best_match, min_error_percent = None, float('inf')
            
            for name, real_mass in pdg_masses_list:
                error_percent = (abs(predicted_mass - real_mass) / real_mass) * 100
                if error_percent < min_error_percent:
                    min_error_percent = error_percent
                    best_match = name
            
            if min_error_percent <= MATCH_THRESHOLD_PERCENT:
                results.append({
                    'channel': p_data['channel'],
                    'predicted_mass': predicted_mass,
                    'match_name': best_match,
                    'error': min_error_percent
                })
        
        if results:
            df = pd.DataFrame(results).sort_values('predicted_mass').reset_index(drop=True)
            print(f"({grid_size}x{grid_size} raw data, error < {MATCH_THRESHOLD_PERCENT:.1f}%)")
            print(f"{'Sim. Channel':<20} | {'Predicted Mass (MeV)':<25} | {'Closest Real Particle':<28} | {'Error (%)':<10}")
            print("-" * 90)
            for _, row in df.iterrows():
                print(f"{row['channel']:<20} | {row['predicted_mass']:<25.2f} | {row['match_name']:<28} | {row['error']:.2f}%")
        else:
            print(f"No matching particles found within a {MATCH_THRESHOLD_PERCENT:.1f}% error margin in the raw {grid_size}x{grid_size} data.")


    
    def _analyze_feynman_spectrum(self, bivector_ts, dt, bivector_indices):
        """
        Analyzes the time series from the Feynman experiment with more sensitive peak detection.
        The threshold is determined by the median of the noise, rather than its maximum.
        """
        all_excitations = {}
        
        for i in range(bivector_ts.shape[0]):
            signal_cpu = bivector_ts[i, :]
            blade_index = bivector_indices[i]
            
            signal_gpu = cp.asarray(signal_cpu)
    
            if not cp.all(cp.isfinite(signal_gpu)) or signal_gpu.size < 50:
                continue
    
            signal_gpu = detrend(signal_gpu, type='linear')
            signal_gpu -= cp.mean(signal_gpu)
    
            if cp.all(cp.abs(signal_gpu) < 1e-12): # Increase sensitivity
                continue
    
            N = len(signal_gpu)
            xf_gpu = cp.fft.rfftfreq(N, dt)
            power_gpu = cp.abs(cp.fft.rfft(signal_gpu))**2
            
            if xf_gpu.size > 0:
                power_gpu[xf_gpu < np.max(xf_gpu)*0.001] = 0 # Low-frequency filter
            
            if not power_gpu.size > 0 or cp.max(power_gpu).get().item() < 1e-20:
                continue
                
            power_cpu = power_gpu.get()
            xf_cpu = xf_gpu.get()
            
            # CHANGE: Peak detection is now based on the median of the noise,
            # not its maximum. This is much better at finding weak signals.
            median_power = np.median(power_cpu[power_cpu > 0])
            if median_power < 1e-20: continue # Meaningless spectrum
                
            prominence_threshold = median_power * 50 # Look for peaks 50 times above the median noise
            peaks, props = find_peaks(power_cpu, prominence=prominence_threshold, distance=20)
            
            for peak_idx in peaks:
                freq = xf_cpu[peak_idx]
                peak_power = props['prominences'][list(peaks).index(peak_idx)]
                
                if freq not in all_excitations or peak_power > all_excitations[freq]['power']:
                    all_excitations[freq] = {
                        'power': peak_power, 
                        'channel': f'bivector_blade_{blade_index}'
                    }
                    
        if not all_excitations:
            return []
            
        return sorted([{'freq': freq, 'power': info['power'], 'channel': info['channel']}
                       for freq, info in all_excitations.items()],
                      key=lambda p: p['freq'])

  
    
    def _track_particles(self, all_particles_by_size):
        if not all_particles_by_size: return {}
        sorted_sizes = sorted(all_particles_by_size.keys())
        if not sorted_sizes: return {}

        base_particles = all_particles_by_size[sorted_sizes[0]]
        particle_tracks = {i: [{'particle': p, 'size': sorted_sizes[0]}] for i, p in enumerate(base_particles)}

        for i in range(1, len(sorted_sizes)):
            current_size = sorted_sizes[i]
            current_particles = all_particles_by_size[current_size]
            unmatched_current = list(current_particles)

            for track_id, track_data in particle_tracks.items():
                last_particle_in_track = track_data[-1]['particle']
                best_match, min_dist = None, float('inf')

                for p_current in unmatched_current:
                    if p_current['channel'] == last_particle_in_track['channel']:
                        dist = abs(p_current['freq'] - last_particle_in_track['freq'])
                        if dist < min_dist:
                            min_dist, best_match = dist, p_current
                
                if best_match and min_dist < (best_match['freq'] * 0.25): 
                    track_data.append({'particle': best_match, 'size': current_size})
                    if best_match in unmatched_current:
                        unmatched_current.remove(best_match)
            
            for p_new in unmatched_current:
                new_track_id = max(particle_tracks.keys()) + 1 if particle_tracks else 0
                particle_tracks[new_track_id] = [{'particle': p_new, 'size': current_size}]

        return particle_tracks
    
    # === THE METHOD CAUSING THE ERROR IS DEFINED IN THIS SECTION ===
    # Make sure this method is correctly indented within your class definition.
    def _plot_extrapolation_report(self, particle_tracks, extrapolated_tracks, electron_candidate_track):
        print("\n" + "-"*38 + " PROOF 3: EXTRAPOLATION GRAPH " + "-"*37)
        
        if not extrapolated_tracks or electron_candidate_track is None:
            print("Not enough extrapolation data available to plot the graph.")
            return

        tracks_to_plot = {}
        
        e_track_id = next(tid for tid, track in particle_tracks.items() if track == electron_candidate_track)
        tracks_to_plot['Electron Candidate'] = {'track': electron_candidate_track, 'id': e_track_id}

        fittable_tracks = []
        for track_id, track in particle_tracks.items():
            if len(track) >= 3 and track_id in extrapolated_tracks: 
                sizes = [d['size'] for d in track]
                masses = [d['particle']['freq'] for d in track]
                x = 1 / (np.array(sizes)**2)
                y = np.array(masses)
                _, _, r_value, _, _ = linregress(x, y)
                fittable_tracks.append({'id': track_id, 'r_squared': r_value**2, 'track': track})

        sorted_tracks = sorted(fittable_tracks, key=lambda t: t['r_squared'], reverse=True)
        
        for track_info in sorted_tracks:
            if len(tracks_to_plot) >= 3: break
            if track_info['id'] != e_track_id:
                grade_str = track_info['track'][0]['particle']['channel']
                grade_int = int(grade_str.split('_')[-1])
                channel_name = self._get_particle_prefix(grade_int)
                label = f"{channel_name} (R²={track_info['r_squared']:.2f})"
                tracks_to_plot[label] = track_info

        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(figsize=(12, 8))
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(tracks_to_plot)))
        
        for i, (label, track_info) in enumerate(tracks_to_plot.items()):
            track = track_info['track']
            sizes = np.array([d['size'] for d in track])
            masses = np.array([d['particle']['freq'] for d in track])
            x = 1 / (sizes**2)
            
            slope, intercept, _, _, _ = linregress(x, masses)
            
            ax.scatter(x, masses, color=colors[i], s=80, zorder=5, label=label)
            
            x_fit = np.array([0, np.max(x)])
            y_fit = slope * x_fit + intercept
            ax.plot(x_fit, y_fit, color=colors[i], linestyle='--', lw=2)
            
            ax.text(0.000005, intercept, f'{intercept:.3f}', fontsize=10, color=colors[i], ha='left', va='center')

        ax.set_xlabel('1 / (Grid Size)² (← Continuous Space Limit)', fontsize=14)
        ax.set_ylabel('Simulated Mass (Frequency)', fontsize=14)
        ax.set_title('Extrapolation of Selected Particle Tracks to the Continuous Space Limit', fontsize=16)
        ax.axvline(0, color='gray', linestyle='-', lw=2)
        ax.legend(fontsize=12)
        ax.grid(True)
        
        plot_filename = os.path.join(self.results_dir, "extrapolation_report.png")
        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
        print(f"✅ Extrapolation graph successfully saved to '{plot_filename}'.")
        plt.show()

    def _plot_spectrum_report(self, grid_size, particles, scaling_factor):
        print(f"\n" + "-"*30 + f" PROOF 4: {grid_size}x{grid_size} MASS SPECTRUM GRAPH " + "-"*30)
        
        if not particles:
            print(f"No particles found for this grid size to plot a graph.")
            return

        masses = [p['freq'] * scaling_factor for p in particles]
        amplitudes = [p['power'] for p in particles]
        grades = [int(p['channel'].split('_')[-1]) for p in particles]
        
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(figsize=(18, 10))
        
        unique_grades = sorted(list(set(grades)))
        colors = plt.cm.turbo(np.linspace(0, 1, len(unique_grades)))
        grade_color_map = {grade: color for grade, color in zip(unique_grades, colors)}

        for grade in unique_grades:
            grade_masses = [m for m, g in zip(masses, grades) if g == grade]
            grade_amps = [a for a, g in zip(amplitudes, grades) if g == grade]
            
            markerline, stemlines, baseline = ax.stem(
                grade_masses, grade_amps, label=self._get_particle_prefix(grade),
                linefmt='-', markerfmt='o', basefmt=' '
            )
            plt.setp(markerline, 'color', grade_color_map[grade])
            plt.setp(stemlines, 'color', grade_color_map[grade], 'linewidth', 1.5, alpha=0.7)

        ax.set_xscale('log')
        ax.set_yscale('log')
        
        ax.set_xlabel('Predicted Mass (MeV) - Logarithmic Scale', fontsize=14)
        ax.set_ylabel('Signal Power (Amplitude) - Logarithmic Scale', fontsize=14)
        ax.set_title(f'Particle Mass Spectrum ({grid_size}x{grid_size})', fontsize=16)
        ax.legend(title="Channel of Origin (Grade)", fontsize=10)
        ax.grid(True, which="both", ls="--")
        
        top_particles = sorted(particles, key=lambda p: p['power'], reverse=True)[:5]
        for p in top_particles:
            mass = p['freq'] * scaling_factor
            amp = p['power']
            ax.text(mass, amp, f' {mass:.1f} MeV', verticalalignment='bottom', fontsize=9)
            
        plot_filename = os.path.join(self.results_dir, f"spectrum_report_{grid_size}x{grid_size}.png")
        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
        print(f"✅ Spectrum graph successfully saved to '{plot_filename}'.")
        plt.show()

    def _extrapolate_to_continuous(self, sizes, masses):
        if len(sizes) < 2: return np.nan, (None, None)
        x = 1 / (np.array(sizes)**2)
        y = np.array(masses)
        try:
            slope, intercept, r_value, _, _ = linregress(x, y)
            # ### CORRECTION: Making the R-squared (goodness-of-fit) threshold more flexible ###
            # This will allow more particle tracks to be considered "valid".
            if len(x) >= 3 and r_value**2 < 0.65: # Reduced from 0.80 to 0.65
                return np.nan, (None, None)
            return intercept, (slope, intercept)
        except ValueError:
            return np.nan, (None, None)

    def _get_particle_prefix(self, grade):
        return self._GRADE_PREFIX_MAP.get(grade, f'G{grade}')

    def run_full_analysis(self):
        print("🚀 Initiating Project Da Vinci...")
        with tqdm(total=len(self.grid_scales), desc="Overall Progress", position=0, leave=True) as pbar_main:
            for size in self.grid_scales:
                pbar_main.set_description(f"Processing Grid {size}x{size}")
                
                noise_for_this_grid = self.config.get('base_noise_levels', {}).get(size, 7e-7)
                pbar_main.write(f"🔧 [{size}x{size}] Noise Level set to: {noise_for_this_grid:.2e}.")
                
                engine = AresLegacyEngine(size, self.alg, self.params, noise_for_this_grid)
                initial_psi = None
                vacuum_filename = os.path.join(self.results_dir, f"vacuum_state_{size}x{size}.npy")
                
                if os.path.exists(vacuum_filename):
                    try:
                        pbar_main.write(f"✅ [{size}x{size}] Found saved equilibrium state, loading...")
                        initial_psi = cp.asarray(np.load(vacuum_filename))
                    except Exception as e:
                        pbar_main.write(f"⚠️ Could not load equilibrium state: {e}. It will be regenerated.")
                        initial_psi = None
                
                if initial_psi is None:
                    pbar_main.write(f"🌊 Generating dynamic equilibrium state for [{size}x{size}]...")
                    current_config = self.config.copy()
                    override_config = self.config.get('scale_overrides', {}).get(size, {})
                    current_config.update(override_config)
                    max_eq_steps = current_config['relax_steps']
                    
                    with tqdm(total=max_eq_steps, desc=f"Final Relaxation ({size}x{size})", position=1, leave=False) as pbar_eq:
                        initial_psi = engine.relax_to_plateau(
                            max_steps=max_eq_steps, pbar=pbar_eq
                        )
                    
                    if initial_psi is not None:
                        pbar_main.write(f"💾 [{size}x{size}] Saving equilibrium state...")
                        np.save(vacuum_filename, initial_psi.get())

                if initial_psi is None:
                    pbar_main.write(f"❌ Could not generate equilibrium state on {size}x{size} grid. Skipping this grid.")
                    pbar_main.update(1)
                    continue

                pbar_main.write(f"🔬 [{size}x{size}] Analyzing the structure of the emergent vacuum...")
                analyzer = EmergentPhenomenaAnalyzer(self.alg, initial_psi)
                symmetry_score, report = analyzer.analyze_symmetry_breaking()
                complexity_score = analyzer.analyze_structural_complexity()
                self.all_results[size]['vacuum_analysis'] = {
                    'symmetry_score': symmetry_score, 'complexity_score': complexity_score, 'grade_norms': report
                }
                
                results_filename = os.path.join(self.results_dir, f"results_grid_{size}.pkl")
                
                if os.path.exists(results_filename):
                    pbar_main.write(f"✅ [{size}x{size}] Found saved experiment results. Skipping experiments.")
                    with open(results_filename, 'rb') as f:
                        self.all_results[size]['experiments'] = pickle.load(f)
                else:
                    self.all_results[size]['experiments'] = {}
                    pbar_main.write(f"🎧 [{size}x{size}] Listening to the system's natural oscillations...")
                    
                    probe_multivector = cp.zeros((1, 1, self.alg.num_blades), dtype=DTYPE_COMPLEX)
                    for g in range(1, self.alg.dim + 1):
                        indices = self.alg.blade_indices.get(g)
                        if indices is not None and indices.size > 0:
                            first_blade_of_grade = indices[0]
                            # As the grade decreases (becomes more fundamental), the probing force increases.
                            probe_multivector[0, 0, first_blade_of_grade] = 1.0 / float(g)
                    # ==========================================================

                    # SMART ANALYSIS (TWO-STAGE)
                    # 1. Lightweight Particle Hunt
                    light_config = {'total_evolution_time': 200.0, 'num_samples': 50000}
                    pbar_main.write(f"  ├─🔬 Stage 1/2: Lightweight Particle Hunt...")
                    # Removed erroneous self._create_probe_mv call
                    ts_light, dt_light = engine.evolve_optimized(initial_psi, probe_multivector, **light_config)
                    particles_light = EmergentPhenomenaAnalyzer.analyze_particle_spectrum_hybrid(ts_light, dt_light)
                    
                    # 2. Heavyweight Hunt
                    heavy_config = {'total_evolution_time': 200.0, 'num_samples': 4000000}
                    pbar_main.write(f"  └─🔭 Stage 2/2: Heavyweight Hunt...")
                    # Removed erroneous self._create_probe_mv call
                    ts_heavy, dt_heavy = engine.evolve_optimized(initial_psi, probe_multivector, **heavy_config)
                    particles_heavy = EmergentPhenomenaAnalyzer.analyze_particle_spectrum_hybrid(ts_heavy, dt_heavy)
                    
                    # 3. Combine Results
                    final_particle_map = {p['freq']: p for p in particles_light}
                    for p in particles_heavy: final_particle_map[p['freq']] = p
                    all_particles_from_run = list(final_particle_map.values())
                    
                    # Distribute results according to the old format
                    for g in range(self.alg.dim + 1):
                        experiment_name = f"{self._get_particle_prefix(g)} Channel Analysis"
                        grade_particles = [p for p in all_particles_from_run if p['channel'] == f'grade_{g}']
                        self.all_results[size]['experiments'][experiment_name] = {
                            'probe_grade': g, 'particles': sorted(grade_particles, key=lambda p: p['freq'])
                        }
                        
                    pbar_main.write(f"💾 [{size}x{size}] Saving experiment results...")
                    with open(results_filename, 'wb') as f:
                        pickle.dump(self.all_results[size]['experiments'], f)

                pbar_main.update(1)
                del engine
                cp.get_default_memory_pool().free_all_blocks()
            
        return self._run_post_analysis_and_generate_report()

    def _run_post_analysis_and_generate_report(self):
        print("\n\n" + "="*95)
        print("### PROJECT DA VINCI II - FINAL VERIFICATION REPORT ###".center(95))
        print(f"Constitution ID: {self.params.get('id', 'N/A')}".center(95))
        print("="*95)
        
        dominant_lambdas = sorted([(k, v) for k, v in self.params.items() if 'lambda_' in k and v > 0.01], key=lambda item: item[1], reverse=True)[:5]
        dominant_lambda_str = ", ".join([f"{k.replace('lambda_','')}={v:.2f}" for k,v in dominant_lambdas])
        
        avg_symmetry = np.mean([res.get('vacuum_analysis', {}).get('symmetry_score', 0) for res in self.all_results.values()])
        
        
        all_particles_by_size = {size: [p for exp_data in res.get('experiments', {}).values() for p in exp_data.get('particles', [])] for size, res in self.all_results.items() if res}
        particle_tracks = self._track_particles(all_particles_by_size)
        
        lightest_freq, electron_candidate_track, extrapolated_tracks, plotting_data, scaling_factor = float('inf'), None, {}, [], None
        for track_id, track in particle_tracks.items():
            if len(track) >= 2:
                # <--- CORRECTION
                sizes = [d['size'] for d in track]; masses = [d['particle']['freq'] for d in track]
                extrapolated_freq, _ = self._extrapolate_to_continuous(sizes, masses)
                if not np.isnan(extrapolated_freq) and extrapolated_freq > 0 and extrapolated_freq < lightest_freq:
                    lightest_freq, electron_candidate_track = extrapolated_freq, track
        
        if electron_candidate_track:
            scaling_factor = PDG_DATA['Electron']['mass'] / lightest_freq
            for track_id, track in particle_tracks.items():
                if len(track) >= 2:
                    # <--- CORRECTION
                    sizes = [d['size'] for d in track]; masses = [d['particle']['freq'] for d in track]
                    extrapolated_freq, (slope, intercept) = self._extrapolate_to_continuous(sizes, masses)
                    if not np.isnan(extrapolated_freq) and extrapolated_freq > 0:
                        channel_str = track[0]['particle']['channel']
                        grade_int = int(channel_str.split('_')[-1])
                        channel_name = self._get_particle_prefix(grade_int)
                        extrapolated_tracks[track_id] = {'mass': extrapolated_freq, 'channel': channel_name}
                        
        num_stable_particles = len(extrapolated_tracks)
        
        alpha_error_percent = self._calculate_and_present_alpha_from_params(self.params) 
        
        print("\nEXECUTIVE SUMMARY:")
        summary = (
            f"This parameter set is a mathematical manifestation of the intrinsic tension created by the axioms of 'Ontological Closure (Σ[Ψ]≡0)' and 'Forced Geometric Asymmetry.' "
            f"The relaxation process has revealed a unique dynamic equilibrium state, which we have named the 'Mandatory Cycle,' adopted by the system under this tension.\n\n"
            f"The analysis of this cycle has demonstrated the model's predictive power through two fundamental proofs:\n\n"
            f"1. Particle Spectrum: The natural resonances of the cycle have produced {num_stable_particles} stable "
            f"phenomenological patterns (particle tracks) consistent with the Standard Model.\n\n"
            f"2. Fundamental Constants: The model's deepest claim, the principle of 'derivability of constants from geometry,' has been tested. "
            f"The Fine-Structure Constant (α) was derived from the ratio of the system's scalar potential strength (λ_p_g0) to its dynamic stiffness (λ_d). "
            f"This ratio was scaled not by the standard '4π' constant of 3D space, but by a geometric constant of '(8/9) * 4π,' arising from the dimensions ('8' and '9') of the model's mathematical arena, the Cl(1,8) algebra. "
            f"When the same parameters that correctly predict the particle spectrum were inserted into this formula, they predicted the Fine-Structure Constant with a deviation of only {alpha_error_percent:.4f}% from its known value.\n\n"
            f"This dual verification strongly suggests that the 'Mandatory Cycle of Existence' paradigm is not only phenomenologically consistent but also offers a profound, testable, and geometric explanation for the origin of the universe's fundamental constants."
        )
        print(summary)
        
        print("\n" + "-"*40 + " PROOF 1: EMERGENT VACUUM STRUCTURE " + "-"*40)
        avg_grade_norms = pd.DataFrame([res.get('vacuum_analysis', {}).get('grade_norms', {}) for res in self.all_results.values()]).mean().to_dict()
        dominant_grades = sorted([(self._get_particle_prefix(int(g.split('_')[-1])), p) for g, p in avg_grade_norms.items() if p > 1.0], key=lambda item: item[1], reverse=True)
        dominant_grades_str = ", ".join([f"{name} ({p:.1f}%)" for name, p in dominant_grades])
        print(f"- Average Symmetry Breaking Score: {avg_symmetry:.2f} (A high value indicates strong breaking)")
        print(f"- Dominant Fields (Average): {dominant_grades_str}")
        print("\n" + "-"*36 + " PROOF 2: EMERGENT PARTICLE SPECTRUM " + "-"*35)
        
        
        
        if scaling_factor:
            print(f"⚖️ Mass Scaling: Lightest stable particle (Electron candidate) found at frequency {lightest_freq:.4f}.")
            print(f"  -> Scale Factor (MeV/frequency): {scaling_factor:.4f}")
            self._present_particle_table(extrapolated_tracks, scaling_factor)
            
            for grid_size, particles_raw in sorted(all_particles_by_size.items()):
                # Continue if there is particle data for that grid
                if particles_raw:
                    # Print the raw data table
                    self._present_raw_particle_table(grid_size, particles_raw, scaling_factor)
                    
                    # Plot the spectrum graph for that grid size
                    self._plot_spectrum_report(grid_size, particles_raw, scaling_factor)
        else:
            print("❌ Reference particle (Electron candidate) for mass scaling could not be found.")

        self._plot_extrapolation_report(particle_tracks, extrapolated_tracks, electron_candidate_track)
        
        print("\n🎉 All analyses and reporting completed.")
        
        return None
    
    
    def _present_particle_table(self, extrapolated_tracks, scaling_factor):
        MATCH_THRESHOLD_PERCENT = 1.0
        
        # Step 1: Find all matches and group them in a dictionary
        # { 'Muon': [{'mass': 104.66, 'channel': 'Grade5'}, {'mass': 104.67, 'channel': 'Vector'}...], ... }
        unified_results = {name: [] for name in PDG_DATA.keys()}
        unmatched_predictions = []

        for track_id, p_data in extrapolated_tracks.items():
            predicted_mass = p_data['mass'] * scaling_factor
            best_match_name, min_error_percent = None, float('inf')

            # Find the best PDG match for each prediction
            for pdg_name, pdg_info in PDG_DATA.items():
                error = (abs(predicted_mass - pdg_info['mass']) / pdg_info['mass']) * 100
                if error < min_error_percent:
                    min_error_percent = error
                    best_match_name = pdg_name
            
            # If the match is below the threshold, add it to the list of the corresponding particle
            if min_error_percent <= MATCH_THRESHOLD_PERCENT:
                unified_results[best_match_name].append({
                    'predicted_mass': predicted_mass,
                    'channel': p_data['channel']
                })
            else: # If it doesn't match, store it as a "new particle candidate"
                unmatched_predictions.append({
                    'predicted_mass': predicted_mass,
                    'channel': p_data['channel'],
                    'closest_match': best_match_name,
                    'distance_percent': min_error_percent
                })

        # Step 2: Print the Unified and Summarized Table
        print(f"\n--- Particles Matched with Standard Model (Summary Report, Error < {MATCH_THRESHOLD_PERCENT}%) ---")
        header = f"{'Matched Particle':<20} | {'Count':<15} | {'Average Mass (MeV)':<25} | {'Std. Dev. (MeV)':<20} | {'Channels of Origin'}"
        print(header)
        print("-" * len(header))

        found_particles_list = []
        for pdg_name, results_list in sorted(unified_results.items(), key=lambda item: PDG_DATA[item[0]]['mass']):
            if not results_list:
                continue
            
            found_particles_list.append(pdg_name)
            masses = [r['predicted_mass'] for r in results_list]
            channels = sorted(list(set(r['channel'] for r in results_list))) # Unique channels
            
            count = len(masses)
            avg_mass = np.mean(masses)
            std_dev = np.std(masses)
            
            # Shorten the channel list if it's too long
            channels_str = ", ".join(channels)
            if len(channels_str) > 40:
                channels_str = channels_str[:37] + "..."

            print(f"{pdg_name:<20} | {count:<15} | {avg_mass:<25.2f} | {std_dev:<20.2f} | {channels_str}")

        # Step 3: List the Genuinely Missing Particles
        print("\n" + "-"*30)
        print("📊 FINAL SPECTRUM ANALYSIS")
        print(f"✅ Unique Particles Found: {len(found_particles_list)} / {len(PDG_DATA)}")
        
        missing_particles = sorted(list(set(PDG_DATA.keys()) - set(found_particles_list)))
        if missing_particles:
            print(f"❌ MISSING PARTICLES: {', '.join(missing_particles)}")
        else:
            print("🎉 PERFECT RESULT: ALL particles in the Standard Model were found!")
        print("-" * 30)
        

    

    

    
# ======================================================================================
# SECTION 3: MAIN EXECUTION BLOCK
# ======================================================================================

if __name__ == '__main__':
    champion_params_v3 = {
        'id': 'champion_v3_axiomatic',
        'lambda_d': 1.9053886991667701, 
        'lambda_complexity': 0.28389757318757647, 
        'damping_relax': 1.4152606616948762, 
        'damping_evolve': 9.99000376642322e-06, 
        'lambda_p_global': 0.10141891690970965, 
        'lambda_p_g0': 0.15596348885683858, 
        'lambda_p_g1': -0.1250819829348453, 
        'lambda_p_g2': -0.037109756279247307, 
        'lambda_p_g3': -0.1735486748174691, 
        'lambda_p_g4': -0.052272306357990954, 
        'lambda_p_g5': -0.040746958432598915, 
        'lambda_p_g6': -0.025715164431201597, 
        'lambda_p_g7': -0.15709054525591912, 
        'lambda_p_g8': -0.13438713505225147, 
        'lambda_p_g9': -0.2741136587586725, 
        'lambda_b_g0': 0.09045044550548677, 
        'lambda_b_g1': 9.90561430294791e-05, 
        'lambda_b_g2': 5.342876510675676e-05, 
        'lambda_b_g3': -4.981938811085131e-06, 
        'lambda_b_g4': 4.5124699927102784e-05, 
        'lambda_b_g5': 0.012409691896710338, 
        'lambda_b_g6': 0.004318538283733796, 
        'lambda_b_g7': 0.00010166944590403118, 
        'lambda_b_g8': 0.0011751611999323325, 
        'lambda_b_g9': 0.254173601310313, 
        'b_field_strength': 2.0
    }
    grid_scales_to_test = [64, 96, 128]
    base_config = {
        'relax_steps': 700000,
        'evolve_time': 200.0,
        'num_samples': 50000,
                
        # NEWLY ADDED SECTION: Grid-specific noise levels
        'base_noise_levels': {
            64: 8.6e-7,  # Noise level for 64x64 grid
            96: 5.6e-7,  # Noise level for 96x96 grid
            128: 4.20e-7 # Noise level for 128x128 grid
        },
        
        'scale_overrides': {}
    }
    alg = CliffordAlgebra(1, 8)
    
    print("="*80 + "\n>>> INITIATING DA VINCI FINAL VERIFICATION PHASE... <<<\n" + "="*80)
    
    # 1. Set up the Da Vinci laboratory
    davinci_lab = ProjectDaVinci(champion_params_v3, grid_scales_to_test, base_config, alg)
    
    # 2. Run all analyses (particle spectrum + alpha calculation)
    davinci_lab.run_full_analysis()
    
    print("\n" + "="*80)
    print(">>> ALL OPERATIONS COMPLETED SUCCESSFULLY. <<<\n" + "="*80)
